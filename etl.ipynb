{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data wrangling with Spark SQL\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/song-data.json\n"
     ]
    }
   ],
   "source": [
    "song_data = \"data/song-data.json\"\n",
    "print(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df = spark.read.json(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df.createOrReplaceTempView(\"staging_songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+------------------+----+---------+\n",
      "|           song_id|           title|         artist_id|year| duration|\n",
      "+------------------+----------------+------------------+----+---------+\n",
      "|SOMZWCG12A8C13C480|I Didn't Mean To|ARD7TVE1187B99BFB1|   0|218.93179|\n",
      "+------------------+----------------+------------------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT \n",
    "  song_id\n",
    ", title\n",
    ", artist_id\n",
    ", year\n",
    ", duration \n",
    "FROM staging_songs \n",
    "LIMIT 2\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_table = spark.sql('''\n",
    "SELECT \n",
    "  song_id\n",
    ", title\n",
    ", artist_id\n",
    ", year\n",
    ", duration \n",
    "FROM staging_songs \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+------------------+----+---------+\n",
      "|           song_id|           title|         artist_id|year| duration|\n",
      "+------------------+----------------+------------------+----+---------+\n",
      "|SOMZWCG12A8C13C480|I Didn't Mean To|ARD7TVE1187B99BFB1|   0|218.93179|\n",
      "+------------------+----------------+------------------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = \"/data/song_table.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "song_table.write.partitionBy(\"year\", \"artist_id\").parquet(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_table = spark.sql('''\n",
    "SELECT \n",
    "  artist_id\n",
    ", artist_name\n",
    ", artist_location\n",
    ", artist_latitude\n",
    ", artist_longitude \n",
    "FROM staging_songs \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+---------------+---------------+----------------+\n",
      "|         artist_id|artist_name|artist_location|artist_latitude|artist_longitude|\n",
      "+------------------+-----------+---------------+---------------+----------------+\n",
      "|ARD7TVE1187B99BFB1|     Casual|California - LA|           null|            null|\n",
      "+------------------+-----------+---------------+---------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artist_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = \"/data/artist_table.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_table.write.parquet(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/log-data.json\n"
     ]
    }
   ],
   "source": [
    "log_data = \"data/log-data.json\"\n",
    "print(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = spark.read.json(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Harmonia', auth='Logged In', firstName='Ryan', gender='M', itemInSession=0, lastName='Smith', length=655.77751, level='free', location='San Jose-Sunnyvale-Santa Clara, CA', method='PUT', page='NextSong', registration=1541016707796.0, sessionId=583, song='Sehr kosmisch', status=200, ts=1542241826796, userAgent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"', userId='26')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.createOrReplaceTempView(\"staging_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table = spark.sql('''\n",
    "SELECT \n",
    "  userId\n",
    ", firstName\n",
    ", lastName\n",
    ", gender\n",
    ", level \n",
    "FROM staging_logs \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId='26', firstName='Ryan', lastName='Smith', gender='M', level='free')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_table.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = \"/data/user_table.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table.write.parquet(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = spark.sql('''\n",
    "SELECT ts from staging_logs limit 1''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"get_timestamp\", lambda x: datetime.datetime.fromtimestamp(x/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_df = spark.sql('''\n",
    "SELECT \n",
    "  get_timestamp(ts) as ts\n",
    "FROM staging_logs \n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ts='java.util.GregorianCalendar[time=?,areFieldsSet=false,areAllFieldsSet=false,lenient=true,zone=sun.util.calendar.ZoneInfo[id=\"Etc/UTC\",offset=0,dstSavings=0,useDaylight=false,transitions=0,lastRule=null],firstDayOfWeek=1,minimalDaysInFirstWeek=1,ERA=?,YEAR=2018,MONTH=10,WEEK_OF_YEAR=?,WEEK_OF_MONTH=?,DAY_OF_MONTH=15,DAY_OF_YEAR=?,DAY_OF_WEEK=?,DAY_OF_WEEK_IN_MONTH=?,AM_PM=0,HOUR=0,HOUR_OF_DAY=0,MINUTE=30,SECOND=26,MILLISECOND=796,ZONE_OFFSET=?,DST_OFFSET=?]')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_df.createOrReplaceTempView(\"staging_ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"get_time\", lambda x: datetime.datetime.fromtimestamp(x/1000).isoformat())\n",
    "spark.udf.register(\"get_hour\", lambda x: int(datetime.datetime.fromtimestamp(x/1000).hour))\n",
    "spark.udf.register(\"get_day\", lambda x: int(datetime.datetime.fromtimestamp(x/1000).day))\n",
    "spark.udf.register(\"get_week\", lambda x: str(datetime.datetime.fromtimestamp(x/1000).isocalendar()[1]))\n",
    "spark.udf.register(\"get_month\", lambda x: int(datetime.datetime.fromtimestamp(x/1000).month))\n",
    "spark.udf.register(\"get_year\", lambda x: int(datetime.datetime.fromtimestamp(x/1000).year))\n",
    "spark.udf.register(\"get_weekday\", lambda x: str(datetime.datetime.fromtimestamp(x/1000).weekday()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table = spark.sql('''\n",
    "SELECT \n",
    "  get_time(ts) as start_time\n",
    ", get_hour(ts) as hour\n",
    ", get_day(ts) as day\n",
    ", get_week(ts) as week\n",
    ", get_month(ts) as month\n",
    ", get_year(ts) as year\n",
    ", get_weekday(ts) as weekday\n",
    "FROM staging_logs\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = \"/data/time_table.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table.write.parquet(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplay_table = spark.sql('''\n",
    "SELECT DISTINCT\n",
    "  S.ARTIST_ID\n",
    ", E.LEVEL\n",
    ", S.ARTIST_LOCATION\n",
    ", E.SESSIONID, S.SONG_ID\n",
    ", get_time(e.ts) as start_time\n",
    ", E.USERAGENT\n",
    ", E.USERID\n",
    ", S.duration\n",
    "FROM STAGING_logS E\n",
    "JOIN STAGING_SONGS S\n",
    "ON E.song = S.title\n",
    "AND E.artist = S.artist_name\n",
    "WHERE E.PAGE = 'NextSong'\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songplay_table.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = \"/data/song_play\"\n",
    "songplay_table.write.partitionBy(\"start_time\").parquet(dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
